{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85c7b4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/soham2312/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/soham2312/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model report: OneVsRestClassifier(estimator=KNeighborsClassifier()): \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00         3\n",
      "           2       1.00      0.80      0.89         5\n",
      "           3       1.00      1.00      1.00         9\n",
      "           4       1.00      1.00      1.00         6\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       0.82      1.00      0.90         9\n",
      "           7       1.00      0.71      0.83         7\n",
      "           8       1.00      0.91      0.95        11\n",
      "           9       1.00      1.00      1.00         9\n",
      "          10       1.00      1.00      1.00         8\n",
      "          11       0.90      1.00      0.95         9\n",
      "          12       1.00      1.00      1.00         5\n",
      "          13       1.00      1.00      1.00         9\n",
      "          14       1.00      1.00      1.00         7\n",
      "          15       1.00      1.00      1.00        19\n",
      "          16       1.00      1.00      1.00         3\n",
      "          17       1.00      1.00      1.00         4\n",
      "          18       1.00      1.00      1.00         5\n",
      "          19       0.86      1.00      0.92         6\n",
      "          20       1.00      1.00      1.00        11\n",
      "          21       1.00      1.00      1.00         4\n",
      "          22       1.00      1.00      1.00        13\n",
      "          23       1.00      1.00      1.00        15\n",
      "          24       1.00      1.00      1.00         8\n",
      "\n",
      "    accuracy                           0.98       193\n",
      "   macro avg       0.98      0.98      0.98       193\n",
      "weighted avg       0.98      0.98      0.98       193\n",
      "\n",
      "\n",
      "[20 14  6 17 15 14 10 14 15 11  6 23  4 11 13  4 19  8  8  9 12 11 17 22\n",
      " 19 16  5  8  3 19 20 18 22  7 23 23 22 18  6 20 10 20 14  8 15 15  8 11\n",
      "  4 22  1 24 14 15 22 23  8 15  3 17 18  3  0 15 15 15 16 21 13 18 12 23\n",
      " 22 12 13 22  8  6 19  4 24 14  7  1 24 13 12 10  9  8 22  9 23 11  9 23\n",
      " 11 15 23 13  4 17  2  5  6 10  0 19 20 10 22 10 15 10 15 15 22  6 14  6\n",
      "  0  4  5  7  9 13 23  6  9  9 21 11  5  3  9 24 19 13  8  3 13 13 11 20\n",
      " 16 23 21 24  7 21 20 15 22 19 15 23  9 15 15  6  2 20  7 11 23 24  8  3\n",
      " 20  2 10 22 15  2 11 23  1 23  6  3  3 24 24 12  5 23 18 22 20 20  3  6\n",
      " 15]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data=pd.read_csv('/Users/soham2312/Downloads/UpdatedResumeDataSet.csv')\n",
    "# print(data['Category'].value_counts())\n",
    "\n",
    "import re\n",
    "\n",
    "def clean(text):\n",
    "    text=re.sub('http\\S+\\s*', ' ', text)\n",
    "    text=re.sub('RT|cc', ' ', text)\n",
    "    text=re.sub('#\\S+', '', text)\n",
    "    text=re.sub('@\\S+', '', text)\n",
    "    text=re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
    "    text=re.sub('\\s+', ' ', text)\n",
    "    text=re.sub(r'[^\\x00-\\x7f]', r' ', text)\n",
    "    return text\n",
    "\n",
    "data['clean text']=data.Resume.apply(lambda x: clean(x))\n",
    "# data.drop('Resume', inplace=True, axis=1)\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "stopwords=set(stopwords.words('english')+['``',\"''\"])\n",
    "\n",
    "total_words=[]\n",
    "sentences=data['clean text'].values\n",
    "cleanSentences =\"\"\n",
    "\n",
    "for i in range(0,962):\n",
    "    text=clean(sentences[i])\n",
    "    cleanSentences+=text\n",
    "    words=nltk.word_tokenize(text)\n",
    "    for word in words:\n",
    "        if word not in stopwords and word not in string.punctuation:\n",
    "            total_words.append(word)\n",
    "            \n",
    "word_freq_dist=nltk.FreqDist(total_words)\n",
    "most_common=word_freq_dist.most_common(100)\n",
    "\n",
    "def stem(text):\n",
    "    t=[]\n",
    "    for i in text.split():\n",
    "        t.append(ps.stem(i))\n",
    "    return \" \".join(t)\n",
    "data['clean text']=data['clean text'].apply(stem)\n",
    "data['clean text']=data['clean text'].apply(lambda x:x.lower())\n",
    "\n",
    "s=[]\n",
    "for i in range(len(most_common)):\n",
    "    s.append(most_common[i][0])\n",
    "s=[\" \".join([stem(word) for word in sentence.split(\" \")]) for sentence in s]\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "var=['Category']\n",
    "le=LabelEncoder()\n",
    "\n",
    "for i in var:\n",
    "    data[i]=le.fit_transform(data[i])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "text=data['clean text'].values\n",
    "\n",
    "terget=data['Category'].values\n",
    "# print(s)\n",
    "# print(text)\n",
    "vect=TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    stop_words='english',\n",
    "    max_features=4000)\n",
    "\n",
    "vect.fit(text)\n",
    "\n",
    "\n",
    "Word_feature=vect.transform(text)\n",
    "\n",
    "x_train, x_test, y_train, y_test=train_test_split(Word_feature, terget, random_state=0, test_size=0.2)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model=OneVsRestClassifier(KNeighborsClassifier())\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "prediction=model.predict(x_test)\n",
    "from sklearn import metrics\n",
    "print(\"model report: %s: \\n %s\\n\" % (model, metrics.classification_report(y_test, prediction)))\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1352a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
